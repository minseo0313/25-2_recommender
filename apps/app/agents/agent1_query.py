from __future__ import annotations
import json, os
from typing import Any, Dict, List
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# OpenAI SDK
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# OpenAI í´ë¼ì´ì–¸íŠ¸
if OpenAI is None:
    print("âš ï¸ OpenAI SDKê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install openai`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    client = None
else:
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    client = OpenAI(api_key=OPENAI_API_KEY)

# ì¶œë ¥ ìŠ¤í‚¤ë§ˆ
class ProcessedQueryOut(BaseModel):
    search_query: str = Field(..., description="ìì—°ì–´ ê²€ìƒ‰ë¬¸")
    filters: Dict[str, Any] = Field(..., description="ì¹´í…Œê³ ë¦¬/ê°€ê²©/ì˜ì–‘íƒ€ê¹ƒ í¬í•¨")
    query_embedding: List[float] = Field(..., description="ì„ë² ë”© ë²¡í„°")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM í”„ë¡¬í”„íŠ¸ (ì¿¼ë¦¬ ìƒì„± + ì˜ì–‘ íƒ€ê¹ƒ ì •ì˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM_SYSTEM_PROMPT = """\
ë‹¹ì‹ ì€ ì‹í’ˆ ì¶”ì²œ ì‹œìŠ¤í…œì˜ 'ì¿¼ë¦¬ ì„¤ê³„ì'ë‹¤.
ì‚¬ìš©ìê°€ ì˜¨ë³´ë”©ì—ì„œ ì…ë ¥í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ë§Œë“¤ì–´ë¼:

1) search_query: ë°˜ë“œì‹œ 'êµ¬ë§¤ ëª©ì 'ì´ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ëœ í˜•íƒœë¡œ ìƒì„±í•˜ë¼.
   (ì˜ˆ: "ìš´ë™ í›„ íšŒë³µìš© í”„ë¡œí‹´", "í˜ˆë‹¹ ê´€ë¦¬ìš© ë‹¨ë°±ì§ˆë°”")
2) filter: ì•„ë˜ í‚¤ë§Œ ì‚¬ìš©í•´ë¼.
    - category: ì‚¬ìš©ìê°€ ì§€ì •í–ˆê±°ë‚˜ í•©ë¦¬ì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•œ ìƒìœ„ ì¹´í…Œê³ ë¦¬(ì—†ìœ¼ë©´ ìƒëµ)
    - price_min: ìˆ«ì(float) (ì—†ìœ¼ë©´ ìƒëµ)
    - price_max: ìˆ«ì(float) (ì—†ìœ¼ë©´ ìƒëµ)
    - nutrition_target: êµ¬ë§¤ ëª©ì ì„ ë°˜ì˜í•œ í•˜ë“œ í•„í„° ê·œì¹™(dict)
        * ì˜ˆì‹œ ê·œì¹™(ê°€ì´ë“œ, ìƒí™©ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥):
            - í˜ˆë‹¹ê´€ë¦¬: sugar_max <= 5
            - ì²´ì¤‘ê°ëŸ‰: sugar_max <= 3, calories_max <= 200
            - ê·¼ìœ¡ëŸ‰ ì¦ê°€: protein_min >= 10
            - ê³ ë‹¨ë°±: protein_min >= 15
        * ìˆ«ì ê°’ë§Œ ì‚¬ìš©(ë‹¨ìœ„ í‘œê¸° ê¸ˆì§€) í‚¤ ì˜ˆ: sugar_max, protein_min, calories_max, fat_max, saturated_fat_max, sodium_max

ì£¼ì˜: 
- ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°’ ì„ì˜ ìƒì„± ê¸ˆì§€(íŠ¹íˆ ê°€ê²©). ì‚¬ìš©ì ì…ë ¥ ì—†ìœ¼ë©´ í•´ë‹¹ í‚¤ ìƒëµ.
- ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ JSON ê°ì²´ë§Œ ë°˜í™˜.
"""

LLM_USER_PROMPT_TEMPLATE = """\
[Onboarding]
{user_query_json}

ìœ„ ì§€ì¹¨ì„ ë”°ë¥´ëŠ” JSONë§Œ ë°˜í™˜í•˜ë¼.
í‚¤ëŠ” search_query, filters ë‘ ê°œë§Œ ì¡´ì¬í•´ì•¼ í•œë‹¤.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í´ë°± ê·œì¹™(LLM/ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨ ì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—ì´ì „íŠ¸ êµ¬í˜„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ëª¨ë¸ ê¸°ë³¸ê°’
DEFAULT_CHAT_MODEL = os.getenv("OPENAI_RESPONSES_MODEL", "gpt-4o-mini")
DEFAULT_EMBED_MODEL = os.getenv("OPENAI_EMBEDDINGS_MODEL", "text-embedding-3-small")

if __name__ == "__main__":
    # ê°„ë‹¨ í…ŒìŠ¤íŠ¸ìš© ì˜¨ë³´ë”© ë°ì´í„°
    onboarding_info = {
        "í‚¤ì›Œë“œ": "í”„ë¡œí‹´",
        "ì¹´í…Œê³ ë¦¬": "ê±´ê°•ì‹í’ˆ",
        "ê°€ê²©ëŒ€": "10000ì› ì´í•˜",
        "êµ¬ë§¤ ëª©ì ": "ê±´ê°• ê°„ì‹"
    }

    # user_query_json í˜•íƒœë¡œ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬
    user_prompt = LLM_USER_PROMPT_TEMPLATE.format(
        user_query_json=json.dumps(onboarding_info, ensure_ascii=False)
    )

    print("ğŸš€ LLM í˜¸ì¶œ í…ŒìŠ¤íŠ¸ ì¤‘...")

    response = client.chat.completions.create(
        model=DEFAULT_CHAT_MODEL,
        messages=[
            {"role": "system", "content": LLM_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]
    )

    llm_output = response.choices[0].message.content
    print("LLM ì¶œë ¥ ê²°ê³¼:\n", llm_output)

    # ì„ë² ë”© í…ŒìŠ¤íŠ¸
    print("\nğŸš€ ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
    embedding = client.embeddings.create(
        model=DEFAULT_EMBED_MODEL,
        input=llm_output
    )
    print("ì„ë² ë”© ê¸¸ì´:", len(embedding.data[0].embedding))





